{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\"../models\")\n",
    "from var_autoencoders import encoder,decoder,VAE\n",
    "import torch\n",
    "import trimesh\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation\n",
    "json_dir = \"../../pix3d.json\"\n",
    "with open(json_dir, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>category</th>\n",
       "      <th>img_size</th>\n",
       "      <th>2d_keypoints</th>\n",
       "      <th>mask</th>\n",
       "      <th>img_source</th>\n",
       "      <th>model</th>\n",
       "      <th>model_raw</th>\n",
       "      <th>model_source</th>\n",
       "      <th>3d_keypoints</th>\n",
       "      <th>voxel</th>\n",
       "      <th>rot_mat</th>\n",
       "      <th>trans_mat</th>\n",
       "      <th>focal_length</th>\n",
       "      <th>cam_position</th>\n",
       "      <th>inplane_rotation</th>\n",
       "      <th>truncated</th>\n",
       "      <th>occluded</th>\n",
       "      <th>slightly_occluded</th>\n",
       "      <th>bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img/bed/0001.png</td>\n",
       "      <td>bed</td>\n",
       "      <td>[395, 244]</td>\n",
       "      <td>[[[182.5, 147.09375], [174.5, 225.09375], [16....</td>\n",
       "      <td>mask/bed/0001.png</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_MALM_2/model.obj</td>\n",
       "      <td>None</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_MALM_2/3d_keypoints.txt</td>\n",
       "      <td>model/bed/IKEA_MALM_2/voxel.mat</td>\n",
       "      <td>[[0.7813941591465821, 0.00095539348511137, -0....</td>\n",
       "      <td>[-0.00024347016915001151, 0.09068297313399999,...</td>\n",
       "      <td>35.270398</td>\n",
       "      <td>[-0.7062195301276326, 0.2367305448542897, -0.8...</td>\n",
       "      <td>-0.078517</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[4, 22, 362, 228]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img/bed/0002.png</td>\n",
       "      <td>bed</td>\n",
       "      <td>[1007, 599]</td>\n",
       "      <td>[[[309.29285714285714, 543.9148660714286], [-1...</td>\n",
       "      <td>mask/bed/0002.png</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_MALM_2/model.obj</td>\n",
       "      <td>None</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_MALM_2/3d_keypoints.txt</td>\n",
       "      <td>model/bed/IKEA_MALM_2/voxel.mat</td>\n",
       "      <td>[[0.6331473196939317, 0.08400992130502279, -0....</td>\n",
       "      <td>[0.025652375712099995, 0.0434050556712, 1.1086...</td>\n",
       "      <td>32.378901</td>\n",
       "      <td>[-0.8365679093356918, 0.3969797870961433, -0.6...</td>\n",
       "      <td>-0.107273</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[46, 47, 927, 599]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img/bed/0003.png</td>\n",
       "      <td>bed</td>\n",
       "      <td>[372, 292]</td>\n",
       "      <td>[[[308.0, 202.09375], [-1.0, -1.0], [-1.0, -1....</td>\n",
       "      <td>mask/bed/0003.png</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_HEMNES_1/model.obj</td>\n",
       "      <td>None</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_HEMNES_1/3d_keypoints.txt</td>\n",
       "      <td>model/bed/IKEA_HEMNES_1/voxel.mat</td>\n",
       "      <td>[[0.9337851369875168, 0.004246139989357373, -0...</td>\n",
       "      <td>[0.05160487022685999, 0.0022441725076000067, 0...</td>\n",
       "      <td>39.511348</td>\n",
       "      <td>[-0.33341418148967134, 0.18052455616467913, -0...</td>\n",
       "      <td>-0.019073</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 33, 344, 292]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img/bed/0004.png</td>\n",
       "      <td>bed</td>\n",
       "      <td>[1063, 755]</td>\n",
       "      <td>[[[230.06357142857144, 399.5266517857143], [93...</td>\n",
       "      <td>mask/bed/0004.png</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_HEMNES_1/model.obj</td>\n",
       "      <td>None</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_HEMNES_1/3d_keypoints.txt</td>\n",
       "      <td>model/bed/IKEA_HEMNES_1/voxel.mat</td>\n",
       "      <td>[[0.5029912563246631, 0.09073372306215703, -0....</td>\n",
       "      <td>[0.02463177822849999, 0.0738148517616, 1.09891...</td>\n",
       "      <td>31.871608</td>\n",
       "      <td>[-0.9464253871842163, 0.2661593054097636, -0.4...</td>\n",
       "      <td>-0.120969</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[89, 74, 984, 738]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img/bed/0005.png</td>\n",
       "      <td>bed</td>\n",
       "      <td>[414, 449]</td>\n",
       "      <td>[[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [4...</td>\n",
       "      <td>mask/bed/0005.png</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_MALM_2/model.obj</td>\n",
       "      <td>None</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_MALM_2/3d_keypoints.txt</td>\n",
       "      <td>model/bed/IKEA_MALM_2/voxel.mat</td>\n",
       "      <td>[[0.7883156484286317, -0.04024974826747893, 0....</td>\n",
       "      <td>[-0.205655543756, 0.057996630364799975, 1.6363...</td>\n",
       "      <td>89.131102</td>\n",
       "      <td>[1.1526451872054488, 0.3727803200217579, -1.12...</td>\n",
       "      <td>-0.083036</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[38, 44, 414, 420]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                img category     img_size  \\\n",
       "0  img/bed/0001.png      bed   [395, 244]   \n",
       "1  img/bed/0002.png      bed  [1007, 599]   \n",
       "2  img/bed/0003.png      bed   [372, 292]   \n",
       "3  img/bed/0004.png      bed  [1063, 755]   \n",
       "4  img/bed/0005.png      bed   [414, 449]   \n",
       "\n",
       "                                        2d_keypoints               mask  \\\n",
       "0  [[[182.5, 147.09375], [174.5, 225.09375], [16....  mask/bed/0001.png   \n",
       "1  [[[309.29285714285714, 543.9148660714286], [-1...  mask/bed/0002.png   \n",
       "2  [[[308.0, 202.09375], [-1.0, -1.0], [-1.0, -1....  mask/bed/0003.png   \n",
       "3  [[[230.06357142857144, 399.5266517857143], [93...  mask/bed/0004.png   \n",
       "4  [[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [4...  mask/bed/0005.png   \n",
       "\n",
       "  img_source                              model model_raw model_source  \\\n",
       "0       ikea    model/bed/IKEA_MALM_2/model.obj      None         ikea   \n",
       "1       ikea    model/bed/IKEA_MALM_2/model.obj      None         ikea   \n",
       "2       ikea  model/bed/IKEA_HEMNES_1/model.obj      None         ikea   \n",
       "3       ikea  model/bed/IKEA_HEMNES_1/model.obj      None         ikea   \n",
       "4       ikea    model/bed/IKEA_MALM_2/model.obj      None         ikea   \n",
       "\n",
       "                               3d_keypoints  \\\n",
       "0    model/bed/IKEA_MALM_2/3d_keypoints.txt   \n",
       "1    model/bed/IKEA_MALM_2/3d_keypoints.txt   \n",
       "2  model/bed/IKEA_HEMNES_1/3d_keypoints.txt   \n",
       "3  model/bed/IKEA_HEMNES_1/3d_keypoints.txt   \n",
       "4    model/bed/IKEA_MALM_2/3d_keypoints.txt   \n",
       "\n",
       "                               voxel  \\\n",
       "0    model/bed/IKEA_MALM_2/voxel.mat   \n",
       "1    model/bed/IKEA_MALM_2/voxel.mat   \n",
       "2  model/bed/IKEA_HEMNES_1/voxel.mat   \n",
       "3  model/bed/IKEA_HEMNES_1/voxel.mat   \n",
       "4    model/bed/IKEA_MALM_2/voxel.mat   \n",
       "\n",
       "                                             rot_mat  \\\n",
       "0  [[0.7813941591465821, 0.00095539348511137, -0....   \n",
       "1  [[0.6331473196939317, 0.08400992130502279, -0....   \n",
       "2  [[0.9337851369875168, 0.004246139989357373, -0...   \n",
       "3  [[0.5029912563246631, 0.09073372306215703, -0....   \n",
       "4  [[0.7883156484286317, -0.04024974826747893, 0....   \n",
       "\n",
       "                                           trans_mat  focal_length  \\\n",
       "0  [-0.00024347016915001151, 0.09068297313399999,...     35.270398   \n",
       "1  [0.025652375712099995, 0.0434050556712, 1.1086...     32.378901   \n",
       "2  [0.05160487022685999, 0.0022441725076000067, 0...     39.511348   \n",
       "3  [0.02463177822849999, 0.0738148517616, 1.09891...     31.871608   \n",
       "4  [-0.205655543756, 0.057996630364799975, 1.6363...     89.131102   \n",
       "\n",
       "                                        cam_position  inplane_rotation  \\\n",
       "0  [-0.7062195301276326, 0.2367305448542897, -0.8...         -0.078517   \n",
       "1  [-0.8365679093356918, 0.3969797870961433, -0.6...         -0.107273   \n",
       "2  [-0.33341418148967134, 0.18052455616467913, -0...         -0.019073   \n",
       "3  [-0.9464253871842163, 0.2661593054097636, -0.4...         -0.120969   \n",
       "4  [1.1526451872054488, 0.3727803200217579, -1.12...         -0.083036   \n",
       "\n",
       "   truncated  occluded  slightly_occluded                bbox  \n",
       "0      False     False              False   [4, 22, 362, 228]  \n",
       "1       True      True              False  [46, 47, 927, 599]  \n",
       "2       True     False              False   [0, 33, 344, 292]  \n",
       "3      False     False              False  [89, 74, 984, 738]  \n",
       "4       True     False               True  [38, 44, 414, 420]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pix3d_dataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, max_vertices=2000, max_faces=2000):\n",
    "        self.transform = transform\n",
    "        self.dataframe = dataframe\n",
    "        self.max_vertices = max_vertices\n",
    "        self.max_faces = max_faces\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = \"../../\" + self.dataframe.iloc[idx]['img']\n",
    "        mask_path = \"../../\" + self.dataframe.iloc[idx]['mask']\n",
    "        obj_path = \"../../\" + self.dataframe.iloc[idx]['model']\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        mesh = trimesh.load(obj_path)\n",
    "        mesh_v = mesh.geometry[list(mesh.geometry.keys())[0]]\n",
    "        vertices = np.array(mesh_v.vertices)\n",
    "        faces = np.array(mesh_v.faces)\n",
    "\n",
    "        if vertices.shape[0] < self.max_vertices:\n",
    "            vertices_padded = np.pad(vertices, ((0, self.max_vertices - vertices.shape[0]), (0, 0)), 'constant')\n",
    "        else:\n",
    "            vertices_padded = vertices[:self.max_vertices]\n",
    "        \n",
    "        if faces.shape[0] < self.max_faces:\n",
    "            faces_padded = np.pad(faces, ((0, self.max_faces - faces.shape[0]), (0, 0)), 'constant')\n",
    "        else:\n",
    "            faces_padded = faces[:self.max_faces]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        vertices_padded = torch.tensor(vertices_padded, dtype=torch.float32)\n",
    "        faces_padded = torch.tensor(faces_padded, dtype=torch.long)\n",
    "\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'mask': mask,\n",
    "            'vertices': vertices_padded,\n",
    "            'faces': faces_padded\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "dataset = pix3d_dataset(dataframe=df, transform=transform)\n",
    "dataloader = DataLoader (dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: torch.Size([1, 3, 128, 128])\n",
      "Masks: torch.Size([1, 1, 128, 128])\n",
      "Vertices: torch.Size([1, 2000, 3])\n",
      "Faces: torch.Size([1, 2000, 3])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    images = batch['image']\n",
    "    masks = batch['mask']\n",
    "    vertices = batch['vertices']\n",
    "    faces = batch['faces']\n",
    "    \n",
    "    print(\"Images:\", images.shape)\n",
    "    print(\"Masks:\", masks.shape)\n",
    "    print(\"Vertices:\", vertices.shape)\n",
    "    print(\"Faces:\", faces.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114, 3) (72, 3)\n"
     ]
    }
   ],
   "source": [
    "sample_mesh = trimesh.load(\"../../model/bed/IKEA_BEDDINGE/model.obj\")\n",
    "mesh = sample_mesh.geometry[list(sample_mesh.geometry.keys())[0]]\n",
    "vertices = mesh.vertices\n",
    "faces = mesh.faces\n",
    "print(vertices.shape, faces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "lr = 3e-4\n",
    "batch_size = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE(input_dim=3, hidden_dim=64, latent_dim=128, output_type=\"3D\", output_shape=(1, 32, 32, 32)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "vae = VAE(3, 256, 128).to(device)\n",
    "encoder = vae.encoder\n",
    "decoder = vae.decoder\n",
    "# encoder = encoder(3, 256, 128).to(device)\n",
    "# Decoder = Decoder(128, 256, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as tqdm\n",
    "# for epoch in tqdm.tqdm(range(epochs)):\n",
    "#     for i, batch in enumerate(dataloader):\n",
    "#         images = batch['image'].to(device)\n",
    "#         masks = batch['mask'].to(device)\n",
    "#         vertices = batch['vertices'].to(device)\n",
    "#         faces = batch['faces'].to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         recon_volumes, mu, logvar = model(images)\n",
    "#         print(recon_volumes.shape, images.shape)\n",
    "#         # recon_images = recon_images.view(*images.size())\n",
    "#         recon_images = recon_volumes.view(images.size(0), images.size(1), images.size(2), images.size(3))\n",
    "#         loss = model.loss_function(recon_images, images, mu, logvar)\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if i % 10 == 0:\n",
    "#             print('Epoch[{}/{}], Step [{}/{}], Loss: {:.3f}'\n",
    "#                   .format(epoch + 1, epochs, i + 1, len(dataloader), loss.item()))\n",
    "\n",
    "def train_vae(model, dataloader, epochs, optimizer, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for i, batch in enumerate(tqdm.tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "            images = batch['image'].to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "            vertices = batch['vertices'].to(device)\n",
    "            faces = batch['faces'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_volumes, mu, logvar = model(images)\n",
    "            if model.output_shape is not None:\n",
    "                recon_volumes = recon_volumes.view(-1, *model.output_shape)\n",
    "            loss = model.loss_function(recon_volumes, images, mu, logvar)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 0/10069 [00:00<?, ?it/s]/Users/abhivansh/Downloads/pix3d/byop-2425/results/../models/var_autoencoders.py:108: UserWarning: Using a target size (torch.Size([1, 3, 128, 128])) that is different to the input size (torch.Size([8, 1, 32, 32, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  recon_loss = F.mse_loss(recon_output, target, reduction='mean')\n",
      "Epoch 1/2:   0%|          | 0/10069 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: torch.Size([1, 256, 16, 16])\n",
      "Decoder output shape: torch.Size([8, 1, 32, 32, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (128) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_vae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m, in \u001b[0;36mtrain_vae\u001b[0;34m(model, dataloader, epochs, optimizer, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39moutput_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     recon_volumes \u001b[38;5;241m=\u001b[39m recon_volumes\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39moutput_shape)\n\u001b[0;32m---> 36\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_volumes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogvar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Downloads/pix3d/byop-2425/results/../models/var_autoencoders.py:108\u001b[0m, in \u001b[0;36mVAE.loss_function\u001b[0;34m(self, recon_output, target, mu, logvar)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, recon_output, target, mu, logvar):\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3D\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m         recon_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2D\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    110\u001b[0m         recon_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(recon_output, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:3791\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[1;32m   3793\u001b[0m     expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3794\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (128) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "train_vae(model, dataloader, epochs, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
