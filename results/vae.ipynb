{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\"../models\")\n",
    "from var_autoencoders import encoder,Decoder,VAE\n",
    "import torch\n",
    "import trimesh\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation\n",
    "json_dir = \"../../pix3d.json\"\n",
    "with open(json_dir, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>category</th>\n",
       "      <th>img_size</th>\n",
       "      <th>2d_keypoints</th>\n",
       "      <th>mask</th>\n",
       "      <th>img_source</th>\n",
       "      <th>model</th>\n",
       "      <th>model_raw</th>\n",
       "      <th>model_source</th>\n",
       "      <th>3d_keypoints</th>\n",
       "      <th>voxel</th>\n",
       "      <th>rot_mat</th>\n",
       "      <th>trans_mat</th>\n",
       "      <th>focal_length</th>\n",
       "      <th>cam_position</th>\n",
       "      <th>inplane_rotation</th>\n",
       "      <th>truncated</th>\n",
       "      <th>occluded</th>\n",
       "      <th>slightly_occluded</th>\n",
       "      <th>bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img/bed/0001.png</td>\n",
       "      <td>bed</td>\n",
       "      <td>[395, 244]</td>\n",
       "      <td>[[[182.5, 147.09375], [174.5, 225.09375], [16....</td>\n",
       "      <td>mask/bed/0001.png</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_MALM_2/model.obj</td>\n",
       "      <td>None</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_MALM_2/3d_keypoints.txt</td>\n",
       "      <td>model/bed/IKEA_MALM_2/voxel.mat</td>\n",
       "      <td>[[0.7813941591465821, 0.00095539348511137, -0....</td>\n",
       "      <td>[-0.00024347016915001151, 0.09068297313399999,...</td>\n",
       "      <td>35.270398</td>\n",
       "      <td>[-0.7062195301276326, 0.2367305448542897, -0.8...</td>\n",
       "      <td>-0.078517</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[4, 22, 362, 228]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img/bed/0002.png</td>\n",
       "      <td>bed</td>\n",
       "      <td>[1007, 599]</td>\n",
       "      <td>[[[309.29285714285714, 543.9148660714286], [-1...</td>\n",
       "      <td>mask/bed/0002.png</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_MALM_2/model.obj</td>\n",
       "      <td>None</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_MALM_2/3d_keypoints.txt</td>\n",
       "      <td>model/bed/IKEA_MALM_2/voxel.mat</td>\n",
       "      <td>[[0.6331473196939317, 0.08400992130502279, -0....</td>\n",
       "      <td>[0.025652375712099995, 0.0434050556712, 1.1086...</td>\n",
       "      <td>32.378901</td>\n",
       "      <td>[-0.8365679093356918, 0.3969797870961433, -0.6...</td>\n",
       "      <td>-0.107273</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[46, 47, 927, 599]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img/bed/0003.png</td>\n",
       "      <td>bed</td>\n",
       "      <td>[372, 292]</td>\n",
       "      <td>[[[308.0, 202.09375], [-1.0, -1.0], [-1.0, -1....</td>\n",
       "      <td>mask/bed/0003.png</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_HEMNES_1/model.obj</td>\n",
       "      <td>None</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_HEMNES_1/3d_keypoints.txt</td>\n",
       "      <td>model/bed/IKEA_HEMNES_1/voxel.mat</td>\n",
       "      <td>[[0.9337851369875168, 0.004246139989357373, -0...</td>\n",
       "      <td>[0.05160487022685999, 0.0022441725076000067, 0...</td>\n",
       "      <td>39.511348</td>\n",
       "      <td>[-0.33341418148967134, 0.18052455616467913, -0...</td>\n",
       "      <td>-0.019073</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 33, 344, 292]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img/bed/0004.png</td>\n",
       "      <td>bed</td>\n",
       "      <td>[1063, 755]</td>\n",
       "      <td>[[[230.06357142857144, 399.5266517857143], [93...</td>\n",
       "      <td>mask/bed/0004.png</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_HEMNES_1/model.obj</td>\n",
       "      <td>None</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_HEMNES_1/3d_keypoints.txt</td>\n",
       "      <td>model/bed/IKEA_HEMNES_1/voxel.mat</td>\n",
       "      <td>[[0.5029912563246631, 0.09073372306215703, -0....</td>\n",
       "      <td>[0.02463177822849999, 0.0738148517616, 1.09891...</td>\n",
       "      <td>31.871608</td>\n",
       "      <td>[-0.9464253871842163, 0.2661593054097636, -0.4...</td>\n",
       "      <td>-0.120969</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[89, 74, 984, 738]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img/bed/0005.png</td>\n",
       "      <td>bed</td>\n",
       "      <td>[414, 449]</td>\n",
       "      <td>[[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [4...</td>\n",
       "      <td>mask/bed/0005.png</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_MALM_2/model.obj</td>\n",
       "      <td>None</td>\n",
       "      <td>ikea</td>\n",
       "      <td>model/bed/IKEA_MALM_2/3d_keypoints.txt</td>\n",
       "      <td>model/bed/IKEA_MALM_2/voxel.mat</td>\n",
       "      <td>[[0.7883156484286317, -0.04024974826747893, 0....</td>\n",
       "      <td>[-0.205655543756, 0.057996630364799975, 1.6363...</td>\n",
       "      <td>89.131102</td>\n",
       "      <td>[1.1526451872054488, 0.3727803200217579, -1.12...</td>\n",
       "      <td>-0.083036</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[38, 44, 414, 420]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                img category     img_size  \\\n",
       "0  img/bed/0001.png      bed   [395, 244]   \n",
       "1  img/bed/0002.png      bed  [1007, 599]   \n",
       "2  img/bed/0003.png      bed   [372, 292]   \n",
       "3  img/bed/0004.png      bed  [1063, 755]   \n",
       "4  img/bed/0005.png      bed   [414, 449]   \n",
       "\n",
       "                                        2d_keypoints               mask  \\\n",
       "0  [[[182.5, 147.09375], [174.5, 225.09375], [16....  mask/bed/0001.png   \n",
       "1  [[[309.29285714285714, 543.9148660714286], [-1...  mask/bed/0002.png   \n",
       "2  [[[308.0, 202.09375], [-1.0, -1.0], [-1.0, -1....  mask/bed/0003.png   \n",
       "3  [[[230.06357142857144, 399.5266517857143], [93...  mask/bed/0004.png   \n",
       "4  [[[-1.0, -1.0], [-1.0, -1.0], [-1.0, -1.0], [4...  mask/bed/0005.png   \n",
       "\n",
       "  img_source                              model model_raw model_source  \\\n",
       "0       ikea    model/bed/IKEA_MALM_2/model.obj      None         ikea   \n",
       "1       ikea    model/bed/IKEA_MALM_2/model.obj      None         ikea   \n",
       "2       ikea  model/bed/IKEA_HEMNES_1/model.obj      None         ikea   \n",
       "3       ikea  model/bed/IKEA_HEMNES_1/model.obj      None         ikea   \n",
       "4       ikea    model/bed/IKEA_MALM_2/model.obj      None         ikea   \n",
       "\n",
       "                               3d_keypoints  \\\n",
       "0    model/bed/IKEA_MALM_2/3d_keypoints.txt   \n",
       "1    model/bed/IKEA_MALM_2/3d_keypoints.txt   \n",
       "2  model/bed/IKEA_HEMNES_1/3d_keypoints.txt   \n",
       "3  model/bed/IKEA_HEMNES_1/3d_keypoints.txt   \n",
       "4    model/bed/IKEA_MALM_2/3d_keypoints.txt   \n",
       "\n",
       "                               voxel  \\\n",
       "0    model/bed/IKEA_MALM_2/voxel.mat   \n",
       "1    model/bed/IKEA_MALM_2/voxel.mat   \n",
       "2  model/bed/IKEA_HEMNES_1/voxel.mat   \n",
       "3  model/bed/IKEA_HEMNES_1/voxel.mat   \n",
       "4    model/bed/IKEA_MALM_2/voxel.mat   \n",
       "\n",
       "                                             rot_mat  \\\n",
       "0  [[0.7813941591465821, 0.00095539348511137, -0....   \n",
       "1  [[0.6331473196939317, 0.08400992130502279, -0....   \n",
       "2  [[0.9337851369875168, 0.004246139989357373, -0...   \n",
       "3  [[0.5029912563246631, 0.09073372306215703, -0....   \n",
       "4  [[0.7883156484286317, -0.04024974826747893, 0....   \n",
       "\n",
       "                                           trans_mat  focal_length  \\\n",
       "0  [-0.00024347016915001151, 0.09068297313399999,...     35.270398   \n",
       "1  [0.025652375712099995, 0.0434050556712, 1.1086...     32.378901   \n",
       "2  [0.05160487022685999, 0.0022441725076000067, 0...     39.511348   \n",
       "3  [0.02463177822849999, 0.0738148517616, 1.09891...     31.871608   \n",
       "4  [-0.205655543756, 0.057996630364799975, 1.6363...     89.131102   \n",
       "\n",
       "                                        cam_position  inplane_rotation  \\\n",
       "0  [-0.7062195301276326, 0.2367305448542897, -0.8...         -0.078517   \n",
       "1  [-0.8365679093356918, 0.3969797870961433, -0.6...         -0.107273   \n",
       "2  [-0.33341418148967134, 0.18052455616467913, -0...         -0.019073   \n",
       "3  [-0.9464253871842163, 0.2661593054097636, -0.4...         -0.120969   \n",
       "4  [1.1526451872054488, 0.3727803200217579, -1.12...         -0.083036   \n",
       "\n",
       "   truncated  occluded  slightly_occluded                bbox  \n",
       "0      False     False              False   [4, 22, 362, 228]  \n",
       "1       True      True              False  [46, 47, 927, 599]  \n",
       "2       True     False              False   [0, 33, 344, 292]  \n",
       "3      False     False              False  [89, 74, 984, 738]  \n",
       "4       True     False               True  [38, 44, 414, 420]  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pix3d_dataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, max_vertices=2000, max_faces=2000):\n",
    "        self.transform = transform\n",
    "        self.dataframe = dataframe\n",
    "        self.max_vertices = max_vertices\n",
    "        self.max_faces = max_faces\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = \"../../\" + self.dataframe.iloc[idx]['img']\n",
    "        mask_path = \"../../\" + self.dataframe.iloc[idx]['mask']\n",
    "        obj_path = \"../../\" + self.dataframe.iloc[idx]['model']\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        mesh = trimesh.load(obj_path)\n",
    "        mesh_v = mesh.geometry[list(mesh.geometry.keys())[0]]\n",
    "        vertices = np.array(mesh_v.vertices)\n",
    "        faces = np.array(mesh_v.faces)\n",
    "\n",
    "        if vertices.shape[0] < self.max_vertices:\n",
    "            vertices_padded = np.pad(vertices, ((0, self.max_vertices - vertices.shape[0]), (0, 0)), 'constant')\n",
    "        else:\n",
    "            vertices_padded = vertices[:self.max_vertices]\n",
    "        \n",
    "        if faces.shape[0] < self.max_faces:\n",
    "            faces_padded = np.pad(faces, ((0, self.max_faces - faces.shape[0]), (0, 0)), 'constant')\n",
    "        else:\n",
    "            faces_padded = faces[:self.max_faces]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        vertices_padded = torch.tensor(vertices_padded, dtype=torch.float32)\n",
    "        faces_padded = torch.tensor(faces_padded, dtype=torch.long)\n",
    "\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'mask': mask,\n",
    "            'vertices': vertices_padded,\n",
    "            'faces': faces_padded\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "dataset = pix3d_dataset(dataframe=df, transform=transform)\n",
    "dataloader = DataLoader (dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: torch.Size([4, 3, 128, 128])\n",
      "Masks: torch.Size([4, 1, 128, 128])\n",
      "Vertices: torch.Size([4, 2000, 3])\n",
      "Faces: torch.Size([4, 2000, 3])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    images = batch['image']\n",
    "    masks = batch['mask']\n",
    "    vertices = batch['vertices']\n",
    "    faces = batch['faces']\n",
    "    \n",
    "    print(\"Images:\", images.shape)\n",
    "    print(\"Masks:\", masks.shape)\n",
    "    print(\"Vertices:\", vertices.shape)\n",
    "    print(\"Faces:\", faces.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114, 3) (72, 3)\n"
     ]
    }
   ],
   "source": [
    "sample_mesh = trimesh.load(\"../../model/bed/IKEA_BEDDINGE/model.obj\")\n",
    "mesh = sample_mesh.geometry[list(sample_mesh.geometry.keys())[0]]\n",
    "vertices = mesh.vertices\n",
    "faces = mesh.faces\n",
    "print(vertices.shape, faces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "lr = 3e-4\n",
    "batch_size = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE(3, 256, 128, 32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.BCELoss()\n",
    "encoder = encoder(3, 256, 128).to(device)\n",
    "Decoder = Decoder(128, 256, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x262144 and 16384x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m faces \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfaces\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mreparameterize(mu, logvar)\n\u001b[1;32m     13\u001b[0m recon_images \u001b[38;5;241m=\u001b[39m Decoder(z)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Downloads/pix3d/byop-2425/results/../models/var_autoencoders.py:44\u001b[0m, in \u001b[0;36mencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_mu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_logvar(x)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mu,logvar\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x262144 and 16384x128)"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        images = batch['image'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        vertices = batch['vertices'].to(device)\n",
    "        faces = batch['faces'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        mu, logvar = encoder(images)\n",
    "\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "\n",
    "        recon_images = Decoder(z)\n",
    "\n",
    "        loss = model.loss_function(recon_images, images, mu, logvar)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch[{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch + 1, epochs, i + 1, len(dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
