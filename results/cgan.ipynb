{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import trimesh\n",
    "import scipy.io\n",
    "from torchvision.datasets import ImageFolder\n",
    "import sys\n",
    "sys.path.append(\"../models\")\n",
    "from gan import Discriminator, Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.43324038 -0.07641411  0.10654314]\n",
      " [-0.43324038 -0.07641411  0.10654314]\n",
      " [-0.43324038 -0.07641411  0.10654314]\n",
      " [ 0.43324038 -0.07641411 -0.22230652]\n",
      " [ 0.43324038 -0.07641411  0.10654314]\n",
      " [ 0.43324038 -0.07641411  0.10654314]\n",
      " [ 0.43324038 -0.07641411  0.10654314]\n",
      " [-0.43324038 -0.07641411 -0.22230652]\n",
      " [-0.43324038 -0.07641411 -0.22230652]\n",
      " [-0.43324038 -0.07641411 -0.22230652]\n",
      " [-0.43324038 -0.0459173  -0.22230652]\n",
      " [-0.43324038 -0.0459173  -0.22230652]\n",
      " [-0.43324038 -0.0459173  -0.22230652]\n",
      " [-0.43324038 -0.04256963 -0.22186802]\n",
      " [-0.43324038 -0.04256963 -0.22186802]\n",
      " [-0.43324038 -0.04256963 -0.22186802]\n",
      " [-0.43324038 -0.039453   -0.2205761 ]\n",
      " [-0.43324038 -0.039453   -0.2205761 ]\n",
      " [-0.43324038 -0.039453   -0.2205761 ]\n",
      " [-0.43324038 -0.03677486 -0.21852035]\n",
      " [-0.43324038 -0.03677486 -0.21852035]\n",
      " [-0.43324038 -0.03677486 -0.21852035]\n",
      " [-0.43324038 -0.03471911 -0.21584221]\n",
      " [-0.43324038 -0.03471911 -0.21584221]\n",
      " [-0.43324038 -0.03471911 -0.21584221]\n",
      " [-0.43324038 -0.03342719 -0.21272558]\n",
      " [-0.43324038 -0.03342719 -0.21272558]\n",
      " [-0.43324038 -0.03342719 -0.21272558]\n",
      " [-0.43324038 -0.0329887  -0.20937791]\n",
      " [-0.43324038 -0.0329887  -0.20937791]\n",
      " [-0.43324038 -0.0329887  -0.20937791]\n",
      " [-0.43324038 -0.0329887   0.10654314]\n",
      " [-0.43324038 -0.0329887   0.10654314]\n",
      " [-0.43324038 -0.0329887   0.10654314]\n",
      " [ 0.43324038 -0.0329887   0.10654314]\n",
      " [ 0.43324038 -0.0329887   0.10654314]\n",
      " [ 0.43324038 -0.0329887   0.10654314]\n",
      " [ 0.43324038 -0.0329887  -0.20937791]\n",
      " [ 0.43324038 -0.0329887  -0.20937791]\n",
      " [ 0.43324038 -0.0329887  -0.20937791]\n",
      " [ 0.43324038 -0.03342719 -0.21272558]\n",
      " [ 0.43324038 -0.03342719 -0.21272558]\n",
      " [ 0.43324038 -0.03342719 -0.21272558]\n",
      " [ 0.43324038 -0.03471911 -0.21584221]\n",
      " [ 0.43324038 -0.03471911 -0.21584221]\n",
      " [ 0.43324038 -0.03471911 -0.21584221]\n",
      " [ 0.43324038 -0.03677486 -0.21852035]\n",
      " [ 0.43324038 -0.03677486 -0.21852035]\n",
      " [ 0.43324038 -0.03677486 -0.21852035]\n",
      " [ 0.43324038 -0.039453   -0.2205761 ]\n",
      " [ 0.43324038 -0.039453   -0.2205761 ]\n",
      " [ 0.43324038 -0.039453   -0.2205761 ]\n",
      " [ 0.43324038 -0.04256963 -0.22186802]\n",
      " [ 0.43324038 -0.04256963 -0.22186802]\n",
      " [ 0.43324038 -0.04256963 -0.22186802]\n",
      " [ 0.43324038 -0.0459173  -0.22230652]\n",
      " [ 0.43324038 -0.0459173  -0.22230652]\n",
      " [ 0.43324038 -0.04535622  0.11817511]\n",
      " [ 0.43324038 -0.04535622  0.11817511]\n",
      " [ 0.43324038 -0.04535622  0.11817511]\n",
      " [-0.43324038  0.17560416  0.17738644]\n",
      " [-0.43324038 -0.04535622  0.11817511]\n",
      " [-0.43324038 -0.04535622  0.11817511]\n",
      " [-0.43324038 -0.04535622  0.11817511]\n",
      " [ 0.43324038  0.17560416  0.17738644]\n",
      " [ 0.43324038  0.17560416  0.17738644]\n",
      " [ 0.43324038  0.17560416  0.17738644]\n",
      " [ 0.43324038  0.18349712  0.14793166]\n",
      " [ 0.43324038  0.18349712  0.14793166]\n",
      " [ 0.43324038  0.18349712  0.14793166]\n",
      " [ 0.43324038  0.18394033  0.14457927]\n",
      " [ 0.43324038  0.18394033  0.14457927]\n",
      " [ 0.43324038  0.18394033  0.14457927]\n",
      " [ 0.43324038  0.18349712  0.14123632]\n",
      " [ 0.43324038  0.18349712  0.14123632]\n",
      " [ 0.43324038  0.18349712  0.14123632]\n",
      " [ 0.43324038  0.1822052   0.13811497]\n",
      " [ 0.43324038  0.1822052   0.13811497]\n",
      " [ 0.43324038  0.1822052   0.13811497]\n",
      " [ 0.43324038  0.18015416  0.13543683]\n",
      " [ 0.43324038  0.18015416  0.13543683]\n",
      " [ 0.43324038  0.18015416  0.13543683]\n",
      " [ 0.43324038  0.17747603  0.13338108]\n",
      " [ 0.43324038  0.17747603  0.13338108]\n",
      " [ 0.43324038  0.17747603  0.13338108]\n",
      " [ 0.43324038  0.17435468  0.13209388]\n",
      " [ 0.43324038  0.17435468  0.13209388]\n",
      " [ 0.43324038  0.17435468  0.13209388]\n",
      " [ 0.43324038 -0.0341203   0.07623022]\n",
      " [ 0.43324038 -0.0341203   0.07623022]\n",
      " [ 0.43324038 -0.0341203   0.07623022]\n",
      " [-0.43324038 -0.0341203   0.07623022]\n",
      " [-0.43324038 -0.0341203   0.07623022]\n",
      " [-0.43324038 -0.0341203   0.07623022]\n",
      " [-0.43324038  0.17435468  0.13209388]\n",
      " [-0.43324038  0.17435468  0.13209388]\n",
      " [-0.43324038  0.17435468  0.13209388]\n",
      " [-0.43324038  0.17747603  0.13338108]\n",
      " [-0.43324038  0.17747603  0.13338108]\n",
      " [-0.43324038  0.17747603  0.13338108]\n",
      " [-0.43324038  0.18015416  0.13543683]\n",
      " [-0.43324038  0.18015416  0.13543683]\n",
      " [-0.43324038  0.18015416  0.13543683]\n",
      " [-0.43324038  0.1822052   0.13811497]\n",
      " [-0.43324038  0.1822052   0.13811497]\n",
      " [-0.43324038  0.1822052   0.13811497]\n",
      " [-0.43324038  0.18349712  0.14123632]\n",
      " [-0.43324038  0.18349712  0.14123632]\n",
      " [-0.43324038  0.18349712  0.14123632]\n",
      " [-0.43324038  0.18394033  0.14457927]\n",
      " [-0.43324038  0.18394033  0.14457927]\n",
      " [-0.43324038  0.18394033  0.14457927]\n",
      " [-0.43324038  0.18349712  0.14793166]\n",
      " [-0.43324038  0.18349712  0.14793166]]\n",
      "[[  0   3   4]\n",
      " [  3   0   7]\n",
      " [  1  12   9]\n",
      " [ 12   1  15]\n",
      " [ 15   1  16]\n",
      " [ 16   1  19]\n",
      " [ 19   1  23]\n",
      " [ 23   1  27]\n",
      " [ 27   1  28]\n",
      " [ 28   1  32]\n",
      " [  6  33   2]\n",
      " [ 33   6  35]\n",
      " [  5  38  36]\n",
      " [  5  42  38]\n",
      " [  5  45  42]\n",
      " [  5  47  45]\n",
      " [  5  50  47]\n",
      " [  5  53  50]\n",
      " [  3  53   5]\n",
      " [ 53   3  55]\n",
      " [  3  10  55]\n",
      " [ 10   3   8]\n",
      " [ 13  56  11]\n",
      " [ 56  13  52]\n",
      " [ 18  54  14]\n",
      " [ 54  18  51]\n",
      " [ 21  49  17]\n",
      " [ 49  21  46]\n",
      " [ 20  44  48]\n",
      " [ 44  20  22]\n",
      " [ 24  41  43]\n",
      " [ 41  24  26]\n",
      " [ 25  39  40]\n",
      " [ 39  25  30]\n",
      " [ 29  34  37]\n",
      " [ 34  29  31]\n",
      " [ 57  60  63]\n",
      " [ 60  57  66]\n",
      " [ 58  69  65]\n",
      " [ 69  58  72]\n",
      " [ 72  58  75]\n",
      " [ 75  58  78]\n",
      " [ 78  58  81]\n",
      " [ 81  58  83]\n",
      " [ 83  58  85]\n",
      " [ 85  58  89]\n",
      " [ 59  91  90]\n",
      " [ 91  59  62]\n",
      " [ 61  96  93]\n",
      " [ 61  99  96]\n",
      " [ 61 100  99]\n",
      " [ 61 104 100]\n",
      " [ 61 107 104]\n",
      " [ 61 109 107]\n",
      " [ 60 109  61]\n",
      " [109  60 113]\n",
      " [ 60  68 113]\n",
      " [ 68  60  64]\n",
      " [ 71 112  67]\n",
      " [112  71 111]\n",
      " [ 74 110  70]\n",
      " [110  74 108]\n",
      " [ 73 105 106]\n",
      " [105  73  77]\n",
      " [ 76 102 103]\n",
      " [102  76  80]\n",
      " [ 84 101  79]\n",
      " [101  84  98]\n",
      " [ 82  95  97]\n",
      " [ 95  82  86]\n",
      " [ 87  92  94]\n",
      " [ 92  87  88]]\n"
     ]
    }
   ],
   "source": [
    "mesh = trimesh.load('../../model/bed/IKEA_BEDDINGE/model.obj')\n",
    "mesh_v = list(mesh.geometry.values())[0]\n",
    "vertices = np.array(mesh_v.vertices)\n",
    "faces = np.array(mesh_v.faces)\n",
    "print(vertices)\n",
    "print(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = \"../../pix3d.json\"\n",
    "with open(json_dir, 'r') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pix3d_dataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.transform = transform\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = \"../../\" + self.dataframe.iloc[idx]['img']\n",
    "        mask_path = \"../../\" + self.dataframe.iloc[idx]['mask']\n",
    "        voxel_path = \"../../\" + self.dataframe.iloc[idx]['voxel']\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "        voxel = scipy.io.loadmat(voxel_path)['voxel']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        voxel = torch.tensor(voxel, dtype=torch.float32).unsqueeze(0)\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'mask': mask,\n",
    "            'voxel': voxel\n",
    "        }\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "hidden_dim = 64\n",
    "lr = 0.0002\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "dataset = pix3d_dataset(df, transform = transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "generator = Generator(img_dim=3, hidden_dim=hidden_dim, latent_dim=latent_dim, output_dim=128).to(device)\n",
    "discriminator = Discriminator(voxel_dim=1, img_dim=3, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([4, 3, 128, 128])\n",
      "Voxel shape: torch.Size([4, 1, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    real_images = data['image'].to(device)\n",
    "    real_voxels = data['voxel'].to(device)\n",
    "    print(\"Image shape:\", real_images.shape)\n",
    "    print(\"Voxel shape:\", real_voxels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real images shape: torch.Size([32, 3, 128, 128])\n",
      "Real voxels shape: torch.Size([32, 1, 128, 128, 128])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        real_images = data['image'].to(device)\n",
    "        real_voxels = data['voxel'].to(device)\n",
    "\n",
    "        print(f\"Real images shape: {real_images.shape}\")\n",
    "        print(f\"Real voxels shape: {real_voxels.shape}\")\n",
    "\n",
    "        batch_size = real_images.size(0)\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        #trainign Discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "        \n",
    "        #real data\n",
    "        outputs = discriminator(real_voxels, real_images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        \n",
    "        #fake data\n",
    "        z = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_voxels = generator(real_images, z)\n",
    "        \n",
    "        print(f\"Fake voxels shape: {fake_voxels.shape}\")\n",
    "        \n",
    "        outputs = discriminator(fake_voxels.detach(), real_images)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        optimizer_g.zero_grad()\n",
    "        \n",
    "        fake_voxels = generator(real_images, z)\n",
    "        outputs = discriminator(fake_voxels, real_images)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
    "\n",
    "    # Optional: Save models after each epoch\n",
    "    torch.save(generator.state_dict(), f'generator_epoch_{epoch+1}.pth')\n",
    "    torch.save(discriminator.state_dict(), f'discriminator_epoch_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
